# -*- coding: utf-8 -*-
"""DBSCAN_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8kpk3eBOHLsEF1dpXtBtGArvnSpoER_
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, silhouette_samples

# Load the dataset
data = pd.read_csv('twitter_user_data.csv', encoding='ISO-8859-1')

# Step 1: Preprocessing - Scrub and feature engineering
data['tweet_to_retweet_ratio'] = data['tweet_count'] / (data['retweet_count'] + 1)
data['description_length'] = data['description'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)

# Keep only relevant features
data_cleaned = data[['fav_number', 'tweet_count', 'retweet_count', 'description_length', 'tweet_to_retweet_ratio', 'gender', 'profile_yn']]

# Step 1.1: Handle missing values by filling or dropping them
# Fill numeric columns with median and categorical columns with mode
data_cleaned.loc[:, 'fav_number'] = data_cleaned['fav_number'].fillna(data_cleaned['fav_number'].median())
data_cleaned.loc[:, 'tweet_count'] = data_cleaned['tweet_count'].fillna(data_cleaned['tweet_count'].median())
data_cleaned.loc[:, 'retweet_count'] = data_cleaned['retweet_count'].fillna(data_cleaned['retweet_count'].median())
data_cleaned.loc[:, 'description_length'] = data_cleaned['description_length'].fillna(data_cleaned['description_length'].median())
data_cleaned.loc[:, 'tweet_to_retweet_ratio'] = data_cleaned['tweet_to_retweet_ratio'].fillna(data_cleaned['tweet_to_retweet_ratio'].median())
data_cleaned.loc[:, 'gender'] = data_cleaned['gender'].fillna(data_cleaned['gender'].mode()[0])
data_cleaned.loc[:, 'profile_yn'] = data_cleaned['profile_yn'].fillna(data_cleaned['profile_yn'].mode()[0])

# Ensure no NaN values remain
data_cleaned.dropna(inplace=True)

# Encode categorical variables
data_cleaned.loc[:, 'gender_encoded'] = data_cleaned['gender'].map({'male': 0, 'female': 1, 'brand': 2})
data_cleaned.loc[:, 'profile_yn_encoded'] = data_cleaned['profile_yn'].map({'yes': 1, 'no': 0})

# Step 2: Scaling
# Using 7 features as requested
features = ['fav_number', 'tweet_count', 'retweet_count', 'description_length',
            'tweet_to_retweet_ratio', 'gender_encoded', 'profile_yn_encoded']
X = data_cleaned[features].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Double check for NaN values in the scaled data
if np.isnan(X_scaled).any():
    print("There are still NaN values in the data. Fixing them.")
    X_scaled = np.nan_to_num(X_scaled)  # Convert any remaining NaNs to 0 or other number

# Step 3: Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(X_scaled)

# Add the cluster labels to the dataset
data_cleaned['cluster'] = clusters

# Step 4: Visualization - PCA for 2D representation of clusters
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)
data_cleaned['pca_one'] = pca_result[:, 0]
data_cleaned['pca_two'] = pca_result[:, 1]

plt.figure(figsize=(10, 6))
sns.scatterplot(x='pca_one', y='pca_two', hue='cluster', palette='tab10', data=data_cleaned, legend='full')
plt.title('DBSCAN Clustering of Twitter Users')
plt.show()

# Step 5: Similar output to classification code
# Plot the distribution of clusters
plt.figure(figsize=(10, 6))
sns.countplot(x='cluster', data=data_cleaned)
plt.title('Cluster Distribution')
plt.show()

# Step 6: Confusion Matrix equivalent for clusters (cluster count per gender)
plt.figure(figsize=(10, 6))
sns.countplot(x='cluster', hue='gender', data=data_cleaned)
plt.title('Cluster Distribution by Gender')
plt.show()

# Step 7: PCA Plot showing clusters colored by gender
plt.figure(figsize=(10, 6))
sns.scatterplot(x='pca_one', y='pca_two', hue='gender', style='cluster', palette='Set2', data=data_cleaned)
plt.title('PCA of Twitter Users Clusters (colored by Gender)')
plt.show()

# Step 8: Analyzing cluster characteristics - Mean values of features by cluster
cluster_summary = data_cleaned.groupby('cluster')[features].mean()
print(cluster_summary)

# Step 9: Silhouette Score calculation
# Silhouette Score measures how similar a point is to its own cluster compared to other clusters
if len(set(clusters)) > 1:  # Silhouette score requires more than 1 cluster
    silhouette_avg = silhouette_score(X_scaled, clusters)
    print(f"Silhouette Score: {silhouette_avg}")

    # Interpretation of the Silhouette score
    if silhouette_avg > 0.75:
        print("Excellent clustering! The clusters are very well separated.")
    elif silhouette_avg > 0.5:
        print("Good clustering. The clusters are reasonably well separated.")
    elif silhouette_avg > 0.25:
        print("The clustering is mediocre. There is some overlap between clusters.")
    else:
        print("Poor clustering. The clusters are not well separated.")

    # Calculate silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X_scaled, clusters)

    # Plot Silhouette Scores for each cluster
    plt.figure(figsize=(12, 8))
    y_lower = 10
    for i in range(len(set(clusters))):
        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / len(set(clusters)))
        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)

        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10  # 10 for the space between clusters

    plt.title(f"Silhouette Plot for DBSCAN Clustering (Average Score: {silhouette_avg:.3f})")
    plt.xlabel("Silhouette Coefficient Values")
    plt.ylabel("Cluster Label")
    plt.axvline(x=silhouette_avg, color="red", linestyle="--")
    plt.show()

else:
    print("Not enough clusters for Silhouette Score")

# Step 10: Number of clusters and noise points
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)

print(f'Number of clusters: {n_clusters}')
print(f'Number of noise points: {n_noise}')

# Step 11: Residual equivalent (Analyzing how well clustering separated profiles)
residuals = data_cleaned['gender_encoded'] - clusters
plt.figure(figsize=(8, 6))
sns.histplot(residuals, kde=True)
plt.title('Residuals (difference between Gender and Cluster Labels)')
plt.show()